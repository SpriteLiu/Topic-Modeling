---
title: "Untitled"
author: "Group 2"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Environment
```{r pressure, echo=FALSE}
#Library the packages used in Topic Modeling
library(topicmodels)
library(tidytext)
library(ggplot2)
library(dplyr)
library(janeaustenr)
library(widyr)
library(igraph)
library(ggraph)
#Import data
IMDB <- read.csv("~/Desktop/IMDB Dataset.csv")
IMDB <- tibble(IMDB)
#Add a column "docs"
IMDB <- IMDB  %>%  mutate(docs = c(1:length(IMDB$review)))
data(stop_words)
```

## Before making plots
```{r}
#Find word counts
book_words <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words)%>%
  count(docs, word, sort = TRUE)
total_words <- book_words %>% 
  group_by(docs) %>% 
  summarize(total = sum(n))
book_words_new <- left_join(book_words, total_words)
#Take a brief look at the words
brief_word <- book_words_new%>%count(word)
arrange(brief_word,desc(n))
```
This part shows the word that appears more frequently in statistics. The number of words“movie”, “film”,”bar” are the largest in the chart, but they are fuzzy words that are meaningless for classification, so we need to delete them to make the data clean.
```{r}
#Add more "stop_words" and clean data
stop_words <- rbind(stop_words,c("movie","SMART"),c("br","SMART"),c("film","SMART"),c("time","SMART"),c("story","SMART"),c("people","SMART"),c("bad","SMART"),c("watch","SMART"),c("movies","SMART"),c("characters","SMART"),c("character","SMART"),c("films","SMART"),c("scenes","SMART"),c("scene","SMART"),c("watching","SMART"),c("10","SMART"),c("times","SMART"))
#Do it again and tidy the data
book_words <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_words)%>%
  count(docs, word, sort = TRUE)
total_words <- book_words %>% 
  group_by(docs) %>% 
  summarize(total = sum(n))
book_words_new <- left_join(book_words, total_words)
book_words$docs <- as.character(book_words$docs)
book_words <- tibble(book_words)
#Cast a one-token-per-row table
IMDB_dtm <- book_words %>%
  cast_dtm(docs, word, n)
IMDB_dtm
```
## LDA
```{r}
#Use LDA function to create a 10 topic model
IMDB_lda <- LDA(IMDB_dtm, k = 10, control = list(seed = 1234))
IMDB_lda
```
```{r}
#Examine per-topic-per-word probabilities
IMDB_topics <- tidy(IMDB_lda, matrix = "beta")
IMDB_topics
```
The column beta represents the probability of that term being generated from that topic for that document. It is the probability of that term belonging to that topic. And the value of beta are generally low in our data.

```{r}
#Find the top 5 terms within each docs
IMDB_top_terms <- IMDB_topics %>%
group_by(topic) %>%
slice_max(beta, n = 5) %>% 
ungroup() %>%
arrange(topic, -beta)
IMDB_top_terms
```
This table shows the five most occurring terms for each topic and their beta values accordingly.
```{r}
#Make a ggplot2 visualization
IMDB_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()+
  labs(title = "Top 5 terms in each LDA topic",
       x = expression(beta), y = NULL)
```
This part examine which topics are associated with which description fields and we use gamma to represent the probability that each document belongs in each topic. Probabilities vary a lot in the data frame, and our model has assigned a probability to each description belonging to each topic we constructed from the phrases. 
```{r}
#Examine the probability each document belongs in each topic
IMDB_lda_gamma <- tidy(IMDB_lda,matrix="gamma")
IMDB_lda_gamma
```
This part examine which topics are associated with which description fields and we use gamma to represent the probability that each document belongs in each topic. Probabilities vary a lot in the data frame, and our model has assigned a probability to each description belonging to each topic we constructed from the phrases. 
```{r}
#Distribution of probablities for all topics
ggplot(IMDB_lda_gamma, aes(gamma)) +
  geom_histogram(alpha = 0.8) +
  scale_y_log10() +
  labs(title = "Distribution of probabilities for all topics",
       y = "Number of documents", x = expression(gamma))+
  xlim(0.05,0.3)
```
This distribution shows that documents are being well discriminated as belonging to a topic or not. We can also look at how the probabilities are distributed within each topic. Notice that gamma runs near zero, which means there are many documents that do not belong in each topic and we need to change the number of topic we chose.
```{r}
#Probability for each topic
ggplot(IMDB_lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 4) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))+
  xlim(0.05,0.3)
```
From these charts, we can see that the magnitude of the values in each topic is far from 1 and mostly concentrated around 0. This is because that in the processing of cleaning up the data, we removing the terms “movie”, “actor”,which appear most frequently in the ten topics, in order to make the data cleaner.
## Pair words
```{r}
#Count how many times each pair od words occur together
IMDB_title <- book_words[,-3]
IMDB_title <- tibble(IMDB_title)
IMDB_title <- IMDB_title%>% anti_join(stop_words)
title_word_pairs <-IMDB_title %>% 
  pairwise_count(word, docs, sort = TRUE, upper = FALSE)
title_word_pairs
```
This chart shows the number of the two most frequently occurring words after deleting the meaningless terms.
```{r}
#Plot networks of these words
set.seed(1234)
title_word_pairs %>%
  filter(n >= 1000) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 1) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```
This chart is the visualization of the pairs of words that occur together most often in description fields. This plot displays ”minutes”,”acting”,”plot”,”real” tends to appear more often together.
##Calculating tf-idf
```{r}
#tf_idf
IMDB_tf_idf <- IMDB_title %>% 
  count(docs, word, sort = TRUE) %>%
  bind_tf_idf(word, docs, n)
IMDB_tf_idf %>% 
  arrange(-tf_idf)
```
These are the most improtant words in IMDB measured by tf-idf.
